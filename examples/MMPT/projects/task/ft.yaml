includes: projects/task/default.yaml
# all derived config will be run by fairseq-train.
task_type: sweep_small
fairseq:
  optimization:
    warmup_updates: 122 # copied from roberta glue: https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.glue.md
  checkpoint:
    # save_interval_updates: 512
    # borrowed from Roberta script.
    restore_file: runs/task/checkpoint_best.pt
    reset_optimizer: True
    reset_dataloader: True
    reset_meters: True
